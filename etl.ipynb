{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "filled-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.2\").enableHiveSupport().getOrCreate()\n",
    "#df_spark =spark.read.format('com.github.saurfang.sas.spark').load('data/i94_apr16_sub.sas7bdat')\n",
    "#df_spark.head()\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf ,col,year,month,dayofmonth,hour,weekofyear,date_format\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld,  DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as long\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as t\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cutting-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "def create_spark_session():\n",
    "    \"\"\"\n",
    "    Create the spark session , entry point of the program\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "def read_data(spark,input_data,format='csv',columns='*',test_size=None,**options):\n",
    "    \"\"\"[summary]\n",
    "    This functions reads different data format from spark. \n",
    "    Args:\n",
    "        spark ([obj]): [spark session]\n",
    "        input_data ([String]): [path to the source data to read]\n",
    "        format (str, optional): [file format to read]. Defaults to 'csv'.\n",
    "        columns (str, optional): [columns to read, subset or all columns]. Defaults to '*'.\n",
    "        test_size ([integer], optional): [Number of row to return for debuging purpose]. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    if test_size is None:\n",
    "        df  = spark.read.load(input_data,format=format,**options).select(columns).drop_duplicates()\n",
    "    else:\n",
    "        df  = spark.read.load(input_data,format=format,**options).select(columns).drop_duplicates().show(test_size)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_data(df,output_path,mode=\"overwrite\",out_format='parquet',columns='*',partitionBy=None,**options):\n",
    "    \"\"\"[summary]\n",
    "    This functions save a file to a specified path after it has been processed \n",
    "    Args:\n",
    "        df ([object]): [dataframe to be saved]\n",
    "        output_path ([type]): [destination path where the file will be saved]\n",
    "        mode (str, optional): [mode of saving, overwrite to replace the file if already exists]. Defaults to \"overwrite\".\n",
    "        out_format (str, optional): [format of the ouput file]. Defaults to 'parquet'.\n",
    "        columns (str, optional): [column list to save]. Defaults to '*'.\n",
    "        partitionBy ([type], optional): [describe columns to partition the data by]. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    df.write.save(output_path,mode=mode,format=out_format,partitionBy=partitionBy,**options)\n",
    "\n",
    "\n",
    "def process_airport_data(spark,input_data,output_data,test_size=None,**options):\n",
    "    \"\"\"[summary]\n",
    "    This function process airport data from S3 and extract \n",
    "    relevant information\n",
    "    Args:\n",
    "        spark ([object]): [sparksession, entry point of the program]\n",
    "        input_data ([String]): [path where the data is stored]\n",
    "        output_data ([type]): [destination folder where the final table will be stored]\n",
    "    \"\"\"\n",
    "    \n",
    "    # useful columns for our project\n",
    "    columns=['ident','type','name','elevation_ft','municipality','gps_code','local_code']\n",
    "    \n",
    "    #reading data\n",
    "    airport_df = read_data(spark,input_data,format='csv',columns=columns,test_size=test_size,**options)\n",
    "    #print(airport_df)\n",
    "    print(type(airport_df))\n",
    "    # saving data\n",
    "    save_data(airport_df,output_path,mode=\"overwrite\",out_format='parquet',columns='*',**options)\n",
    "    \n",
    "def process_demographic_data(spark,input_data,output_path,test_size=None,**options):\n",
    "    \"\"\"[summary]\n",
    "    This function process us_demographic data from S3 and extract \n",
    "    relevant information\n",
    "    Args:\n",
    "         spark ([object]): [sparksession, entry point of the program]\n",
    "        input_data ([String]): [path where the data is stored]\n",
    "        output_data ([type]): [destination folder where the final table will be stored]\n",
    "    \"\"\"\n",
    "     #reading data\n",
    "    demographic_df = read_data(spark,input_data,format='csv',columns='*',test_size=test_size,**options)\n",
    "    \n",
    "    #renaming columns\n",
    "    demographic_df = demographic_df.withColumnRenamed('Median Age','median_age').withColumnRenamed('Male Population','male_population')\\\n",
    "                                   .withColumnRenamed('Female Population','Female_population')\\\n",
    "                                   .withColumnRenamed('Total Population','total_population')\\\n",
    "                                   .withColumnRenamed('Number of Veterans','num_veterans')\\\n",
    "                                   .withColumnRenamed('Average Household Size','avg_household_size')\\\n",
    "                                   .withColumnRenamed('State Code','state_code')\\\n",
    "                                   .withColumnRenamed('Race','race')\\\n",
    "                                   .withColumnRenamed('Count','count')\\\n",
    "                                   .withColumnRenamed('City','city')\\\n",
    "                                   .withColumnRenamed('State','state')\n",
    "\n",
    "    columns=['state_code','state','city','median_age','male_population','Female_population','total_population','num_veterans',\\\n",
    "        'foreign_born','avg_household_size','race','count']\n",
    "\n",
    "\n",
    "    # saving data\n",
    "    save_data(demographic_df,output_path,mode=\"overwrite\",out_format='parquet',columns=columns,**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "anticipated-accuracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "input_data=\"./data/airport-codes_csv.csv\" #\"s3a://thim-bucket-2022/data/airport-codes_csv.csv\" #\n",
    "output_path=\"./data/airport.parquet\" #output_path=\"s3a://immigration-schema/airport.parquet\" #\n",
    "process_airport_data(spark,input_data,output_path,test_size=None,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "loose-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=\"./data/us-cities-demographics.csv\" \n",
    "output_path=\"./data/demographic.parquet\"\n",
    "process_demographic_data(spark,input_data,output_path,test_size=None,header=True,delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ancient-entrance",
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadstatError",
     "evalue": "Invalid file, or file has unsupported features",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mReadstatError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5a0f44725c61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#procssing imminration data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyreadstat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./data/i94_apr16_sub.sas7bdat\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mpyreadstat\\pyreadstat.pyx\u001b[0m in \u001b[0;36mpyreadstat.pyreadstat.read_dta\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpyreadstat\\_readstat_parser.pyx\u001b[0m in \u001b[0;36mpyreadstat._readstat_parser.run_conversion\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpyreadstat\\_readstat_parser.pyx\u001b[0m in \u001b[0;36mpyreadstat._readstat_parser.run_readstat_parser\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpyreadstat\\_readstat_parser.pyx\u001b[0m in \u001b[0;36mpyreadstat._readstat_parser.check_exit_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mReadstatError\u001b[0m: Invalid file, or file has unsupported features"
     ]
    }
   ],
   "source": [
    "#procssing imminration data\n",
    "df, meta = pyreadstat.read_dta(\"./data/i94_apr16_sub.sas7bdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-craft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
